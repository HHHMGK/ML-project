{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os, csv\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data (Datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2484 622 777\n"
     ]
    }
   ],
   "source": [
    "movies_train = pd.read_csv('../dataset/movies_train.csv', escapechar='\\\\', quoting=csv.QUOTE_NONE)\n",
    "movies_train, movies_val = train_test_split(movies_train, test_size=0.2, random_state=42)\n",
    "movies_test = pd.read_csv('../dataset/movies_test.csv', escapechar='\\\\', quoting=csv.QUOTE_NONE)\n",
    "print(len(movies_train), len(movies_val), len(movies_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the great muppet caper</td>\n",
       "      <td>[\"Children's\", 'Comedy']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doctor zhivago</td>\n",
       "      <td>['Drama', 'Romance', 'War']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>frankenstein meets the wolf man</td>\n",
       "      <td>['Horror']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>for your eyes only</td>\n",
       "      <td>['Action']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the mirror</td>\n",
       "      <td>['Drama']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>the inheritors</td>\n",
       "      <td>['Drama']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>the karate kid, part ii</td>\n",
       "      <td>['Action', 'Adventure', 'Drama']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>a league of their own</td>\n",
       "      <td>['Comedy', 'Drama']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>algiers</td>\n",
       "      <td>['Drama', 'Romance']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>baby geniuses</td>\n",
       "      <td>['Comedy']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>777 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               title                             genre\n",
       "0             the great muppet caper          [\"Children's\", 'Comedy']\n",
       "1                     doctor zhivago       ['Drama', 'Romance', 'War']\n",
       "2    frankenstein meets the wolf man                        ['Horror']\n",
       "3                 for your eyes only                        ['Action']\n",
       "4                         the mirror                         ['Drama']\n",
       "..                               ...                               ...\n",
       "772                   the inheritors                         ['Drama']\n",
       "773          the karate kid, part ii  ['Action', 'Adventure', 'Drama']\n",
       "774            a league of their own               ['Comedy', 'Drama']\n",
       "775                          algiers              ['Drama', 'Romance']\n",
       "776                    baby geniuses                        ['Comedy']\n",
       "\n",
       "[777 rows x 2 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "TITLE_MAX_LEN = 15\n",
    "pad_token = '<PAD>'\n",
    "unk_token = '<UNK>'\n",
    "\n",
    "def tokenize(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def create_vocab(dataset, column='title'):\n",
    "    df = dataset.copy()\n",
    "    titles = df[column].tolist()\n",
    "    vocab = set()\n",
    "    for title in titles:\n",
    "        tokens = tokenize(title)\n",
    "        vocab.update(tokens)\n",
    "    vocab = list(vocab)\n",
    "    vocab.append(pad_token)\n",
    "    vocab.append(unk_token)\n",
    "    return vocab\n",
    "\n",
    "def onehot_vectorize(title, title2int):\n",
    "    tokens = tokenize(title)\n",
    "    # title_vec = np.zeros(TITLE_MAX_LEN, dtype=np.float32)\n",
    "    # title_vec.fill(title2int[pad_token])\n",
    "    # for i, token in enumerate(tokens):\n",
    "    #     if i >= TITLE_MAX_LEN:  \n",
    "    #         break\n",
    "    #     if token in title2int:\n",
    "    #         title_vec[i] = title2int[token]\n",
    "    #     else:\n",
    "    #         title_vec[i] = title2int[unk_token]\n",
    "    # return title_vec\n",
    "    tokens = tokens[:TITLE_MAX_LEN]\n",
    "    while len(tokens) < TITLE_MAX_LEN:\n",
    "        tokens.append(pad_token)\n",
    "    title_vec = np.zeros((TITLE_MAX_LEN,len(title2int)), dtype=np.float32)\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in title2int:\n",
    "            title_vec[i][title2int[token]] = 1\n",
    "        else:\n",
    "            title_vec[i][title2int[unk_token]] = 1\n",
    "    return title_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_hot_genres(genres,  genres_dict):\n",
    "    genres = genres.strip('][').replace(\"'\", \"\").split(', ')\n",
    "    multi_hot = np.zeros(len(genres_dict))\n",
    "    for genre in genres:\n",
    "        if genre in genres_dict:\n",
    "                multi_hot[genres_dict[genre]] = 1\n",
    "    return multi_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class titleDataset(Dataset):\n",
    "    def __init__(self, df, title2int, genre2dict):\n",
    "        self.df = df\n",
    "        self.title2int = title2int\n",
    "        self.genre2dict = genre2dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        title = self.df.iloc[idx]['title']\n",
    "        title_vec = onehot_vectorize(title, self.title2int)\n",
    "        genres = self.df.iloc[idx]['genre']\n",
    "        genres_vec = multi_hot_genres(genres, self.genre2dict)\n",
    "        return title_vec, genres_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class title_genres_dataset(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size=32, data_folder='../dataset/'):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.data_folder = data_folder\n",
    "        self.prepare_data()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # load data\n",
    "        movies_train = pd.read_csv(self.data_folder + 'movies_train.csv', escapechar='\\\\', quoting=csv.QUOTE_NONE)\n",
    "        movies_test = pd.read_csv(self.data_folder + 'movies_test.csv', escapechar='\\\\', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "        # title process\n",
    "        vocab = create_vocab(movies_train.merge(movies_test), column='title')\n",
    "        self.vocab_size = len(vocab)\n",
    "        title2int = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "        # genres process\n",
    "        genres_list = []\n",
    "        with open(self.data_folder + 'genres.txt', 'r') as f:\n",
    "            genres_list = [g.replace('\\n','') for g in f.readlines()]\n",
    "        genre2int = {genre: i for i, genre in enumerate(genres_list)} \n",
    "\n",
    "        # create dataset\n",
    "        # split train and val\n",
    "        movies_train, movies_val = train_test_split(movies_train, test_size=0.2, random_state=42)\n",
    "        movies_train.reset_index(drop=True, inplace=True)\n",
    "        movies_test.reset_index(drop=True, inplace=True)\n",
    "        movies_val.reset_index(drop=True, inplace=True)\n",
    "        train = titleDataset(movies_train, title2int, genre2int)\n",
    "        val = titleDataset(movies_val, title2int, genre2int)\n",
    "        test = titleDataset(movies_test, title2int, genre2int)\n",
    "        self.movies_train_dataloader = DataLoader(train, batch_size=self.batch_size, shuffle=True)\n",
    "        self.movies_val_dataloader = DataLoader(val, batch_size=self.batch_size, shuffle=False)\n",
    "        self.movies_test_dataloader = DataLoader(test, batch_size=self.batch_size, shuffle=False)\n",
    "        \n",
    "        # movies_train = movies_train.values.tolist()\n",
    "        # movies_val = movies_val.values.tolist()\n",
    "        # movies_test = movies_test.values.tolist()\n",
    "        # self.movies_train_dataloader = DataLoader(movies_train, batch_size=self.batch_size, shuffle=True)\n",
    "        # self.movies_val_dataloader = DataLoader(movies_val, batch_size=self.batch_size, shuffle=False)\n",
    "        # self.movies_test_dataloader = DataLoader(movies_test, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.movies_train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.movies_val_dataloader\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return self.movies_test_dataloader\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model (RNN) define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class titleRNN(pl.LightningModule):\n",
    "    def __init__(self, device=\"cpu\", input_size=15, hidden_size=32, num_layers=2, batch_first=True, bidirectional = True):\n",
    "        super(titleRNN,self).__init__()\n",
    "        self.dev = device # device variable was taken, so using dev instead :(\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=batch_first, nonlinearity='relu', bidirectional=bidirectional)\n",
    "        self.linear = nn.Linear(hidden_size, 18)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        x,_ = self.rnn(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(),lr=0.001)\n",
    "    \n",
    "    def cross_entropy_loss(self,logits,labels):\n",
    "        return F.cross_entropy(logits,labels)\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        title, genres = train_batch\n",
    "        print(\"-----------------------------------\", title.shape, genres.shape)\n",
    "        title_tensor = torch.tensor(title).to(self.dev)\n",
    "        genre_tensor = torch.tensor(genres).to(self.dev)\n",
    "\n",
    "        output = self.forward(title_tensor)\n",
    "        loss = self.cross_entropy_loss(output, genre_tensor)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        title, genres = val_batch\n",
    "        title_tensor = torch.tensor(title).to(self.dev)\n",
    "        genre_tensor = torch.tensor(genres).to(self.dev)\n",
    "\n",
    "        output = self.forward(title_tensor)\n",
    "        loss = self.cross_entropy_loss(output, genre_tensor)\n",
    "        self.log('val_loss', loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 3 fields in line 3, saw 5\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m      3\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m titleDATASET \u001b[38;5;241m=\u001b[39m \u001b[43mtitle_genres_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m titleDATASET\u001b[38;5;241m.\u001b[39mtrain_dataloader()\n\u001b[1;32m      7\u001b[0m val_dataloader \u001b[38;5;241m=\u001b[39m titleDATASET\u001b[38;5;241m.\u001b[39mval_dataloader()\n",
      "Cell \u001b[0;32mIn[92], line 6\u001b[0m, in \u001b[0;36mtitle_genres_dataset.__init__\u001b[0;34m(self, batch_size, data_folder)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m batch_size\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_folder \u001b[38;5;241m=\u001b[39m data_folder\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[92], line 10\u001b[0m, in \u001b[0;36mtitle_genres_dataset.prepare_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# load data\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     movies_train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_folder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmovies_train.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQUOTE_NONE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     movies_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovies_test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, escapechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m, quoting\u001b[38;5;241m=\u001b[39mcsv\u001b[38;5;241m.\u001b[39mQUOTE_NONE)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# title process\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.11/site-packages/pandas/io/parsers/readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m     (\n\u001b[1;32m   1745\u001b[0m         index,\n\u001b[1;32m   1746\u001b[0m         columns,\n\u001b[1;32m   1747\u001b[0m         col_dict,\n\u001b[0;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:904\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2058\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 3 fields in line 3, saw 5\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "titleDATASET = title_genres_dataset(batch_size=BATCH_SIZE)\n",
    "train_dataloader = titleDATASET.train_dataloader()\n",
    "val_dataloader = titleDATASET.val_dataloader()\n",
    "\n",
    "titleModel = titleRNN(device, hidden_size=titleDATASET.vocab_size)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=NUM_EPOCHS, num_sanity_val_steps=0)\n",
    "trainer.fit(titleModel, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `DataLoader`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m titleDATASET\u001b[38;5;241m.\u001b[39mtest_dataloader()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:748\u001b[0m, in \u001b[0;36mTrainer.test\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    745\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Trainer.test()` requires a `LightningModule` when it hasn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt been passed in a previous run\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    746\u001b[0m         )\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 748\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_unwrap_optimized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    749\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    750\u001b[0m _verify_strategy_supports_compile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.11/site-packages/pytorch_lightning/utilities/compile.py:132\u001b[0m, in \u001b[0;36m_maybe_unwrap_optimized\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m    131\u001b[0m _check_mixed_imports(model)\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: `model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `DataLoader`"
     ]
    }
   ],
   "source": [
    "test_dataloader = titleDATASET.test_dataloader()\n",
    "trainer.test(test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
