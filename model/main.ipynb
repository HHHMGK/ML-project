{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.current_device()\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from PIL import Image\n",
    "import os, csv\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_folder = '../dataset/'\n",
    "with open(data_folder + 'genres.txt', 'r') as f:\n",
    "    genres_list = [g.replace('\\n','') for g in f.readlines()]\n",
    "movies_train = pd.read_csv(data_folder + 'movies_train.csv')\n",
    "movies_test = pd.read_csv(data_folder + 'movies_test.csv')\n",
    "movies_val = pd.read_csv(data_folder + 'movies_val.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TITLE_MAX_LEN = 15\n",
    "pad_token = '<PAD>'\n",
    "unk_token = '<UNK>'\n",
    "\n",
    "def tokenize(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def create_vocab(dataset, column='title'):\n",
    "    df = dataset.copy()\n",
    "    titles = df[column].tolist()\n",
    "    vocab = set()\n",
    "    for title in titles:\n",
    "        tokens = tokenize(title)\n",
    "        vocab.update(tokens)\n",
    "    vocab = list(vocab)\n",
    "    vocab.append(pad_token)\n",
    "    vocab.append(unk_token)\n",
    "    return vocab\n",
    "\n",
    "def onehot_vectorize(title, title2int):\n",
    "    tokens = tokenize(title)\n",
    "    tokens = tokens[:TITLE_MAX_LEN]\n",
    "    while len(tokens) < TITLE_MAX_LEN:\n",
    "        tokens.append(pad_token)\n",
    "    title_vec = np.zeros((TITLE_MAX_LEN,len(title2int)), dtype=np.float32)\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in title2int:\n",
    "            title_vec[i][title2int[token]] = 1\n",
    "        else:\n",
    "            title_vec[i][title2int[unk_token]] = 1\n",
    "    return title_vec\n",
    "    \n",
    "def multihot_genres(genres,  genres_dict):\n",
    "    genres = genres.strip('][').replace(\"'\", \"\").split(', ')\n",
    "    multi_hot = np.zeros(len(genres_dict))\n",
    "    for genre in genres:\n",
    "        if genre in genres_dict:\n",
    "                multi_hot[genres_dict[genre]] = 1\n",
    "    return multi_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with different image sizes\n",
    "IMAGE_SIZE_TEST_MAP = {\"16\": (16, 16),\n",
    "                       \"24\": (24, 24),\n",
    "                       \"32\": (32, 32),\n",
    "                       \"40\": (40, 40),\n",
    "                       \"64\": (64, 64),\n",
    "                       \"72\": (72, 72),\n",
    "                       \"128\": (128, 128),\n",
    "                       \"224\": (224, 224),\n",
    "                       \"256\": (256, 256)}\n",
    "IMAGE_SIZE = IMAGE_SIZE_TEST_MAP['224']\n",
    "transformer = transforms.Compose([\n",
    "    transforms.Resize(size=IMAGE_SIZE),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class titleDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "        # title process\n",
    "        vocab = create_vocab(df, column='title')\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.title2int = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "        # image process\n",
    "        self.transformer = transformer\n",
    "\n",
    "        # genres process\n",
    "        genres_list = ['Crime', 'Thriller', 'Fantasy', 'Horror', 'Sci-Fi', 'Comedy', 'Documentary', 'Adventure', 'Film-Noir', 'Animation', 'Romance', 'Drama', 'Western', 'Musical', 'Action', 'Mystery', 'War', \"Children's\"]\n",
    "        self.genre2int = {genre: i for i, genre in enumerate(genres_list)} \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        title = self.df.iloc[idx]['title']\n",
    "        title_vec = onehot_vectorize(title, self.title2int)\n",
    "        genres = self.df.iloc[idx]['genre']\n",
    "        genres_vec = multihot_genres(genres, self.genre2int)\n",
    "        img_path = self.df.iloc[idx]['img_path']\n",
    "        if not os.path.exists(img_path):\n",
    "            img_path = '../dataset/images/0.jpg'\n",
    "        img = Image.open(img_path)\n",
    "        if len(img.getbands()) == 1: # check if the image have only one channel\n",
    "            trans = transforms.Grayscale(num_output_channels=3)\n",
    "            img = trans(img) # convert image to a three-channel image\n",
    "        img = self.transformer(img)\n",
    "        return title_vec, img, genres_vec\n",
    "    \n",
    "    def merge_vocab(self, other):\n",
    "        self.title2int.update(other.title2int)\n",
    "        self.genre2int.update(other.genre2int)\n",
    "        self.vocab_size = len(self.title2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = os.cpu_count()\n",
    "train = titleDataset(movies_train)\n",
    "val = titleDataset(movies_val)\n",
    "test = titleDataset(movies_test)\n",
    "train.merge_vocab(val)\n",
    "train.merge_vocab(test)\n",
    "val.merge_vocab(train)\n",
    "test.merge_vocab(train)\n",
    "train_dataloader = DataLoader(train, batch_size=32, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_dataloader = DataLoader(val, batch_size=32, shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_dataloader = DataLoader(test, batch_size=32, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class theModel(pl.LightningModule):\n",
    "    def __init__(self, titleParam, posterParam, num_labels=18, device='cpu'):\n",
    "        \"\"\"\n",
    "        The main model using LSTM for title and TinyVGG for poster\n",
    "        :param tuple titleParam: (input_size : int, hidden_size : int, num_layers : int, bidirectional : bool)\n",
    "        :param tuple posterParam: (input_shape, hidden_units)\n",
    "        \"\"\"\n",
    "        super(theModel, self).__init__()\n",
    "        self.dev = device  # device variable was taken, so using dev instead :(\n",
    "        self.input_size, self.hidden_size, self.num_layers, self.bidirectional = titleParam\n",
    "        self.input_shape, self.hidden_units = posterParam\n",
    "        self.num_labesls = num_labels\n",
    "\n",
    "        # Title\n",
    "        self.core = nn.LSTM(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "            bidirectional=self.bidirectional\n",
    "        )\n",
    "        linear_size = self.hidden_size\n",
    "        if self.bidirectional:\n",
    "            linear_size *= 2\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(linear_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, self.num_labesls),\n",
    "            # nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Poster\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.input_shape, out_channels=self.hidden_units,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=self.hidden_units,\n",
    "                      out_channels=self.hidden_units, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.hidden_units,\n",
    "                      out_channels=self.hidden_units, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=self.hidden_units,\n",
    "                      out_channels=self.hidden_units, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=self.hidden_units *\n",
    "                      int(IMAGE_SIZE[0]/4)*int(IMAGE_SIZE[0]/4), out_features=1024),\n",
    "            nn.ReLU(),\n",
    "                nn.Linear(in_features=1024, out_features=512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(in_features=512, out_features=256),\n",
    "                nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=self.num_labesls)\n",
    "        )\n",
    "\n",
    "        # Assembling\n",
    "        self.fc = nn.Linear(2*self.num_labesls, self.num_labesls)\n",
    "\n",
    "    def forward(self, title, poster):\n",
    "        # Title\n",
    "        Tout, _ = self.core(title)  # , h0)\n",
    "        # out = [batch_size, seq_len, hidden_size*bidirectional (vector size)]\n",
    "        # => only take the last element (many to one RNN)\n",
    "        Tout = Tout[:, -1, :]\n",
    "        Tout = self.linear(Tout)\n",
    "\n",
    "        # Poster\n",
    "        Pout = self.conv_block_1(poster)\n",
    "        Pout = self.conv_block_2(Pout)\n",
    "        Pout = self.classifier(Pout)\n",
    "        \n",
    "        # Assembling\n",
    "        out = self.fc(torch.cat((Tout, Pout), dim=1))\n",
    "        return out\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        title, img, genres = train_batch\n",
    "        title_tensor = title.clone().detach().to(self.dev)\n",
    "        img_tensor = img.clone().detach().to(self.dev)\n",
    "        genre_tensor = genres.clone().detach().to(self.dev)\n",
    "\n",
    "        output = self.forward(title_tensor, img_tensor)\n",
    "        loss = self.cross_entropy_loss(output, genre_tensor)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        title, img, genres = val_batch\n",
    "        title_tensor = title.clone().detach().to(self.dev)\n",
    "        img_tensor = img.clone().detach().to(self.dev)\n",
    "        genre_tensor = genres.clone().detach().to(self.dev)\n",
    "\n",
    "        output = self.forward(title_tensor, img_tensor)\n",
    "        loss = self.cross_entropy_loss(output, genre_tensor)\n",
    "        self.log('val_loss', loss)\n",
    "        # print('val_loss', loss)\n",
    "\n",
    "    # def test_step(self, test_batch, batch_idx):\n",
    "    #     title, genres = test_batch\n",
    "    #     title_tensor = title.clone().detach().to(self.dev)\n",
    "    #     genre_tensor = genres.clone().detach().to(self.dev)\n",
    "\n",
    "    #     output = self.forward(title_tensor)\n",
    "\n",
    "    #     MAP = MAP_at_K(3, output, genre_tensor)\n",
    "    #     print('MAP@3', MAP)\n",
    "    #     self.log('MAP@3', MAP)\n",
    "    #     return {'MAP@3': MAP}\n",
    "\n",
    "    # def on_test_epoch_end(self, outputs):\n",
    "    #     avg_MAP = torch.stack([x['MAP@3'] for x in outputs]).mean()\n",
    "    #     print('avg_MAP@3', avg_MAP)\n",
    "    #     self.log('avg_MAP@3', avg_MAP)\n",
    "    #     return {'avg_MAP@3': avg_MAP}\n",
    "\n",
    "    def predict_step(self, test_batch, batch_idx):\n",
    "        title, img, genres = test_batch\n",
    "        title_tensor = title.clone().detach().to(self.dev)\n",
    "        img_tensor = img.clone().detach().to(self.dev)\n",
    "        genre_tensor = genres.clone().detach().to(self.dev)\n",
    "\n",
    "        output = self.forward(title_tensor, img_tensor)\n",
    "        return output, genre_tensor\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "    def cross_entropy_loss(self, logits, labels):\n",
    "        return F.cross_entropy(logits, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "4071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type       | Params\n",
      "--------------------------------------------\n",
      "0 | core         | LSTM       | 4.7 M \n",
      "1 | linear       | Sequential | 19.1 K\n",
      "2 | conv_block_1 | Sequential | 10.1 K\n",
      "3 | conv_block_2 | Sequential | 18.5 K\n",
      "4 | classifier   | Sequential | 103 M \n",
      "5 | fc           | Linear     | 666   \n",
      "--------------------------------------------\n",
      "108 M     Trainable params\n",
      "0         Non-trainable params\n",
      "108 M     Total params\n",
      "432.793   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 78/78 [00:19<00:00,  4.05it/s, v_num=81]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 78/78 [00:22<00:00,  3.43it/s, v_num=81]\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(train_dataloader.dataset.vocab_size)\n",
    "titleParam = (train_dataloader.dataset.vocab_size, 128, 2, True)\n",
    "posterParam = (3, 32)\n",
    "model = theModel(titleParam, posterParam, num_labels=18, device=device)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=NUM_EPOCHS, num_sanity_val_steps=0)\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer and Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_at_K(k, pred, truth):\n",
    "    # print(pred)\n",
    "    _, indices = torch.topk(pred, k=k)\n",
    "    correct = 0\n",
    "    for id in indices:\n",
    "        if truth[id] > 0:\n",
    "            correct += 1\n",
    "    return correct / k\n",
    "\n",
    "def AP_at_K(k, pred, truth):\n",
    "    AP = 0\n",
    "    for i in range(1, k+1):\n",
    "        AP += P_at_K(i, pred, truth) \n",
    "    return AP / k\n",
    "\n",
    "def MAP_at_K(k, pred_list, truth_list):\n",
    "    MAP = 0\n",
    "    for i in range(len(pred_list)):\n",
    "        MAP += AP_at_K(k, pred_list[i], truth_list[i])\n",
    "    return MAP / len(pred_list)\n",
    "\n",
    "\n",
    "def normalize(pred, topk=False):\n",
    "    pred_1 = torch.zeros(pred.shape)\n",
    "    for i in range(pred.shape[0]):\n",
    "        if topk:\n",
    "            ids = torch.topk(pred[i], k=3).indices\n",
    "        else:\n",
    "            ids = [j*(pred[i][j] > 0.0) for j in range(len(pred[i]))]\n",
    "        for id in ids:\n",
    "            pred_1[i][id] = 1\n",
    "    return pred_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huy/miniconda3/envs/ml/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:186: .predict(ckpt_path=\"last\") is set, but there is no last checkpoint available. No checkpoint will be loaded.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 25/25 [00:01<00:00, 17.66it/s]\n",
      "torch.Size([777, 18])\n",
      "torch.Size([777, 18])\n",
      "torch.Size([777, 18])\n"
     ]
    }
   ],
   "source": [
    "res = trainer.predict(model, dataloaders=test_dataloader, ckpt_path=\"last\")\n",
    "pred = torch.cat([ep[0] for ep in res])\n",
    "truth = torch.cat([ep[1] for ep in res])\n",
    "\n",
    "pred_1 = normalize(pred, topk=True)\n",
    "print(pred.shape)\n",
    "print(pred_1.shape)\n",
    "print(truth.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "MAP@1  0.39768339768339767\n",
      "MAP@2  0.37773487773487774\n",
      "MAP@3  0.3442013442013438\n",
      "MAP@4  0.3176748176748195\n",
      "--------------------------------------\n",
      "F1 : 0.14092479646205902\n",
      "--------------------------------------\n",
      "Accuracy : 0.08451308310031891\n",
      "--------------------------------------\n",
      "Precision : 0.08451308310031891\n",
      "--------------------------------------\n",
      "Recall : 0.9444444179534912\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------------------------\")\n",
    "print('MAP@1 ', MAP_at_K(1, pred, truth))\n",
    "print('MAP@2 ', MAP_at_K(2, pred, truth))\n",
    "print('MAP@3 ', MAP_at_K(3, pred, truth))\n",
    "print('MAP@4 ', MAP_at_K(4, pred, truth))\n",
    "\n",
    "print(\"--------------------------------------\")\n",
    "# from torchmetrics.functional.classification import multilabel_f1_score\n",
    "from torchmetrics.classification import MultilabelF1Score\n",
    "\n",
    "thres=0.0\n",
    "# print('mF1 - micro:    ', multilabel_f1_score(pred, truth, num_labels=18, threshold=thres, average='micro').tolist())\n",
    "# print('mF1 - macro:    ', multilabel_f1_score(pred, truth, num_labels=18, threshold=thres, average='macro').tolist())\n",
    "# print('mF1 - weighted: ', multilabel_f1_score(pred, truth, num_labels=18, threshold=thres, average='weighted').tolist())\n",
    "\n",
    "f1 = MultilabelF1Score(num_labels=18, threshold=thres, average='macro')\n",
    "print('F1 :', f1(pred, truth).tolist())\n",
    "\n",
    "print(\"--------------------------------------\")\n",
    "# from torchmetrics.functional.classification import multilabel_accuracy\n",
    "from torchmetrics.classification import MultilabelAccuracy\n",
    "acc = MultilabelAccuracy(num_labels=18, threshold=thres)\n",
    "print('Accuracy :', acc(pred, truth).tolist())\n",
    "\n",
    "print(\"--------------------------------------\")\n",
    "# from torchmetrics.functional.classification import multilabel_precision\n",
    "from torchmetrics.classification import MultilabelPrecision\n",
    "prec = MultilabelPrecision(num_labels=18, threshold=thres, average='macro')\n",
    "print('Precision :', prec(pred, truth).tolist())\n",
    "\n",
    "print(\"--------------------------------------\")\n",
    "# from torchmetrics.functional.classification import multilabel_recall\n",
    "from torchmetrics.classification import MultilabelRecall\n",
    "rec = MultilabelRecall(num_labels=18, threshold=thres, average='macro')\n",
    "print('Recall :', rec(pred, truth).tolist())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
